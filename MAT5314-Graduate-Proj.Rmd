---
title: |
  | \vspace{-5em}MAT5314 Graduate Project Team 2
author: 
  - Teng Li(7373086)
  - Shiya Gao(300381032) 
  - Chuhan Yue(300376046)
  - Yang Lyu(8701121)
output: pdf_document
header-includes: 
  - \renewcommand{\and}{\\}
fontsize: 10pt
bibliography: References.bib
link-citations: yes
---

```{r, include=FALSE}
library(tidyverse)
library(kableExtra)
```

# Introduction
Earthquakes are usually caused when the underground rock suddenly breaks and there is rapid motion along a fault. They can be categorized into natural earthquakes and artificial earthquakes. Natural earthquakes can be further classified into tectonic earthquakes and volcanic earthquakes. Tectonic earthquakes are caused by the rupture of rocks deep in the ground and the rapid release of energy accumulated over a long period of time. Volcanic earthquakes are caused by volcanic eruptions. In addition, artificial earthquakes are caused by human activities that alter the stresses and strains on Earth's crust. In our project we will only focus on the natural earthquake due to its probabilistic nature of occurrence.

Earthquakes are difficult to predict for three main reasons. Firstly, plate boundaries are prone to earthquakes, but the way and speed of plate movement is difficult to measure and predict accurately. Secondly, the low frequency of large earthquakes does not provide enough data for earthquake modeling. Thirdly, it is difficult to enter the inner crust of the earth to observe the data, so most of the detection of earthquakes is done by collecting some vibrations and signals on the surface to analyze and predict them, which leads to a decrease in the accuracy of earthquake prediction.

Frequent historical earthquakes give us a huge data set that can be used to study earthquake causes, correlations, space-time, and further earthquake prediction. Current researches have provided several statistical models to analyze the earthquake processes. Trigger model, a special case of the Neyman-Scott clustering model, can be used to estimate aftershocks after a major earthquake [@Cite1]. Epidemic-Type Aftershocks Sequence (ETAS) model is also widely used to forecast earthquake occurrences [@Cite2]. In addition, time series analysis can be done to explore cycles related to earthquake frequency and it is also effective in predicting large earthquakes [@Cite3]. There are also several techniques from machine learning that provide us alternative ways other than conventional statistical models to analyze the earthquake data. For example, deep learning can be used to predict seismic events, including intensity and location [@Cite4]. Clustering model can identify regions with high-frequency earthquakes to upgrade building structures to reduce damage from impending earthquakes.

We focus on a data set of earthquake occurrence in Canada provided by the Government of Canada. The data set is located at https://open.canada.ca/data/en/dataset/4cedd37e-0023-41fe-8eff-bea45385e469. In this project we want to analyze the data to give a detailed review of this natural disaster in the country and make some inference about its characteristics. In particular, we want to check if there's any location-wise pattern in earthquake occurrence and how frequent it can happen. Then we want to analyze if the occurrence of earthquake is also correlated to some other variables such as time. Finally we want to apply some probabilistic models that have been extensively studies by other researchers to the data in order to assess the likelihood of future occurrence in high-risk area.

# Method
First, we will demonstrate some aspects of the earthquake occurrence by visualizing the seismic data. Map charts and other related plots will be used to perform such initial analysis. Then we will use time series analysis to study whether there exist temporal trends, seasonal patterns, and certain frequency in seismic events. Third, we will use geographic characteristics such as longitude, latitude and magnitude to identify potential groups of clusters. We may use different techniques of cluster analysis to achieve this goal. This will allow us to compare the predicted clusters with the actual location of earthquakes. Lastly, we will try to estimate the probability of earthquakes in high-risk regions by using the variables from the data and the Epidemic-Type Aftershocks Sequence (ETAS) model.

```{r, echo=FALSE}
eqarchive <- read.csv("eqarchive-en.csv", header = TRUE)
```

```{r, echo=FALSE}
DataDict<-data.frame(
  Variables=colnames(eqarchive), 
  Example=sapply(eqarchive, function(x) paste(as.character(head(unique(x),1)), collapse = ", ")),
  Meanings=c("The exactly time that earthquakes occur.",
            "The location of a point north or south of the equator.Latitude is shown on a map or globe as east-west lines parallel to the equator.",
            "The location of a point east or west of the prime meridian.Longitude is shown on a map or globe as north-south lines left and right of the prime meridian, which passes through Greenwich, England.",
            "The value given is the depth below the surface of the mean spheroid. It shows the point of initial rupture within Earth",
            "Magnitude is a measure of the amount of energy released during an earthquake.",
            "Distinct magnitude scales base on the type of the earthquakes and the differences of data collection.",
            "The locations where earthquakes happened.",
            "continued for place",
            "continued for place"
            )
)
```

```{r, echo=FALSE}
DataDict %>%
  remove_rownames() %>%
  kable(booktabs = TRUE, caption = "Data Dictionary") %>%
  kable_styling(font_size = 11, latex_options = c("striped", "scale_down", "hold_position")) %>%
  column_spec(3, width = "40em") %>%
  row_spec(0, bold = TRUE)
```


Note that all values in "date" are specified in UTC (Coordinated Universal Time) and of the form: YYYY-MM-DDTHH:MM:SS+ssss, where the prefix before T refers to the date, the suffix after T refers to the specified UTC time and ssss means the sub-seconds expressed between 1/10 of a second to microsecond resolution.  

By exploring variables "magnitude.type", we found out the earthquakes were recoreded in different magnitude scales. 

```{r, echo=FALSE}
cat("*Group by magnitude type:\n")
table(eqarchive$magnitude.type)
```

$M_{L}$ means local magnitude, which is the magnitude defined by Charles Richter for California. However, due to the different attenuation methods of seismic waves in Eastern North America, Otto Nuttli defined a magnitude formula more suitable for Eastern North America, called Magnitude Nuttli, which is $m_{N}$. There exists some other scales of magnitude, but the magnitude of the moment scale ($M_{W}$ or $M$) is the most common way to describe earthquakes. (https://earthquakescanada.nrcan.gc.ca/info-gen/faq-en.php#ml_and_mn)

The main benefit of Mw is that the magnitude is directly tied to earthquake source processes that do not saturate. Body wave magnitudes ($m_{b}$) and surface magnitudes ($M_{s}$) do not correctly reflect the size of large earthquakes due to saturating at large magnitudes.

Negative magnitude is not an error, because magnitude calculations are based on a logarithmic scale. 10 times drop in amplitude decreases the magnitude by 1. For example, magnitude -1 is 10 time less than magnitude 0. Negative magnitude represents small earthquake which are not felt by human. (https://earthquakescanada.nrcan.gc.ca/info-gen/faq-en.php#ml_and_mn)

# References{.allowframebreaks}